{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from sklearn import svm\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from skimage import io, filters\n",
    "from skimage.transform import resize\n",
    "from skimage.feature import hog, ORB, CENSURE, corner_peaks, corner_harris, BRIEF\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "\n",
    "#enter fruit names here: Choose from apple, banana, pineapple, kiwi\n",
    "#example:\n",
    "classes = ['banana', 'pineapple']\n",
    "\n",
    "def main():\n",
    "    DataSet = []\n",
    "    LabelSet = []\n",
    "    lengthV = []\n",
    "    trainPaths = ['./fruit/'+c+ '_train/' for c in classes ]\n",
    "    testPaths =  ['./fruit/'+c+' test/'   for c in classes ]\n",
    "    \n",
    "    resList = []\n",
    "    boolList = []\n",
    "    pos = 0\n",
    "    ind = 0\n",
    "    #if you wish to automatically perform both feature selection optimzation and svm optimization at the same time\n",
    "    #comment out next line and comment in section above\n",
    "    #Warning: Very long runtime for algorithm because of grid search\n",
    "    useList = [True, True, True, True]\n",
    "    #print(useList)\n",
    "    \n",
    "    for c in range(len(classes)):\n",
    "        #get label for features to be added\n",
    "        className = classes[c]\n",
    "        #get file path for folder with images\n",
    "        path = trainPaths[c]\n",
    "        #initialize feature detectors/extractors\n",
    "        #Censure extractor\n",
    "        detector = CENSURE()\n",
    "        #ORB extractor\n",
    "        detector2 = ORB(n_keypoints=50)\n",
    "        #get all file names from the folder\n",
    "        files = os.listdir(path)\n",
    "        nfiles = len(files)\n",
    "        #repeat for each file\n",
    "        for i in range(nfiles):\n",
    "            #initialize feature vector as empty list\n",
    "            featureVector = []\n",
    "            infile = files[i]\n",
    "            #read image as grayscale numpy.ndarray\n",
    "            img = io.imread(path+infile, as_grey=True)\n",
    "            #get histogram for grayscale value intensity\n",
    "            hist = np.histogram(img, bins=256)\n",
    "            #resize image\n",
    "            img = resize(img, (400,400))\n",
    "            #extract features but do not yet add them to feature vector\n",
    "            detector2.detect_and_extract(img)\n",
    "            #extract HOG features, add them to featurevector\n",
    "            a = fd = hog(img, orientations=9, pixels_per_cell=(32, 32),\n",
    "                    cells_per_block=(1,1), visualise=False)\n",
    "            #add histogramm to featurevector\n",
    "            for h in hist:\n",
    "                fd = np.append(fd, h)\n",
    "            #if corresponding boolean in uselist is true add features to featureVector --> Feature selection happens here\n",
    "            if(useList[0]):                            \n",
    "                detector.detect(img)\n",
    "                fd = np.append(fd, [np.array(detector.keypoints).flatten()])\n",
    "            if(useList[1]):\n",
    "                fd = np.append(fd, detector2.keypoints)\n",
    "            if(useList[2]):\n",
    "                fd = np.append(fd, edgeExtract(img, 100))\n",
    "            if(useList[3]):\n",
    "                corners =  corner_peaks(corner_harris(img),min_distance=1)\n",
    "                fd = np.append(fd, corners)\n",
    "            #get length of featurevector for later operations\n",
    "            lengthV.append(len(fd))\n",
    "            #add featureVector list to dataset that is fed into svm\n",
    "            DataSet.append(fd)\n",
    "            #get label name\n",
    "            ind = classes.index(className)\n",
    "            #add label to label dataset that is fed into svm\n",
    "            LabelSet.append(ind)\n",
    "    #get length of biggest sized featurevector\n",
    "    max = np.amax(lengthV)\n",
    "    lengthV = []\n",
    "    DataSet2 = []\n",
    "    #pad dataset with zeroes so that all featurevectors have the same length --> important for svm\n",
    "    for d in DataSet:\n",
    "        d = np.pad(d, (0, max - len(d)), 'constant')\n",
    "        DataSet2.append(d)\n",
    "        lengthV.append(len(d))\n",
    "    DataSet = DataSet2\n",
    "    #perform a grid search with maximum number of possible threads (usually 4)\n",
    "    if __name__=='__main__':\n",
    "        gridSearch(DataSet, LabelSet)\n",
    "    #train and examine svm with default values for comparison later\n",
    "    clf = svm.SVC(kernel='rbf', C=10.0, gamma=1.0000000000000001e-09)\n",
    "    clf.fit(DataSet, LabelSet)\n",
    "    joblib.dump(clf, classes[0]+' '+ classes[1]+'.pk1')\n",
    "    scores = cross_val_score(clf, DataSet, LabelSet, cv=10)\n",
    "    #print results of default svm\n",
    "    print(scores)\n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "#extract edge histogramm, bins number = 100    \n",
    "def edgeExtract(img, bins):\n",
    "    retVal = []\n",
    "    #apply vertical and horizontal sobel filters to get two histogramms, once of vertical and once of horizontal edges\n",
    "    #vertical\n",
    "    fs = filters.sobel_v(img)\n",
    "    #horizontal\n",
    "    angs = filters.sobel_h(img)\n",
    "    #compute histograms\n",
    "    lhist = np.histogram(fs,bins,normed=True,range=(0,1))\n",
    "    ahist = np.histogram(angs, bins,normed=True,range=(-180,180))\n",
    "    #fuse histograms into one list\n",
    "    retVal.extend(lhist[0].tolist())\n",
    "    retVal.extend(ahist[0].tolist())\n",
    "    return retVal\n",
    "#Perform grid search, \n",
    "# if optimum feature selection list is to be found verbalize = false\n",
    "def gridSearch(DataSet, LabelSet, verbalize = True):\n",
    "    #define logspace/interval from which c and gamma valuest are computed and saved to a dictionary to be passed as a parameter\n",
    "    #c from 1e-2 to 1e10\n",
    "    C_range = np.logspace(-2, 10, 13)\n",
    "    #gamma from 1e-9 to 1e3\n",
    "    gamma_range = np.logspace(-9, 3, 13)\n",
    "    param_grid = dict(gamma=gamma_range, C=C_range)\n",
    "    #grid input parameter\n",
    "    cv = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\n",
    "    #perform grid search with multiple threads for added performance\n",
    "    if(verbalize):\n",
    "        grid = GridSearchCV(SVC(), param_grid=param_grid, cv=cv, n_jobs=-1)\n",
    "    #perform grid search with one thread and return value --> multiple threads do not work with return values\n",
    "    else:\n",
    "        grid = GridSearchCV(SVC(), param_grid=param_grid, cv=cv, n_jobs=1)\n",
    "    #find optimal values\n",
    "    grid.fit(DataSet, LabelSet)\n",
    "    #print results of test for optimal values\n",
    "    if(verbalize):\n",
    "        print(\"The best parameters are %s with a score of %0.2f\" \n",
    "            % (grid.best_params_, grid.best_score_))\n",
    "    else:\n",
    "        #return best scores for different feature selections\n",
    "        return grid.best_score_\n",
    "#same as main function, however a sample of 100 images is drawn for each image\n",
    "def selectFeatures(useList):\n",
    "    DataSet = []\n",
    "    LabelSet = []\n",
    "    lengthV = []\n",
    "    trainPaths = ['./fruit/'+c+ '_train/' for c in classes ]\n",
    "    testPaths =  ['./fruit/'+c+' test/'   for c in classes ]\n",
    "    for c in range(len(classes)):\n",
    "        className = classes[c]\n",
    "        path = trainPaths[c]\n",
    "        detector = CENSURE()\n",
    "        detector2 = ORB(n_keypoints=50)\n",
    "        detector3 = BRIEF(patch_size=49)\n",
    "        files = os.listdir(path)\n",
    "        #sample\n",
    "        files = random.sample(files, 100)\n",
    "        nfiles = len(files)\n",
    "        for i in range(nfiles):\n",
    "            featureVector = []\n",
    "            infile = files[i]\n",
    "            img = io.imread(path+infile, as_grey=True)\n",
    "            hist = np.histogram(img, bins=256)\n",
    "            img = resize(img, (400,400))\n",
    "            detector2.detect_and_extract(img)\n",
    "            detector.detect(img)\n",
    "            a = fd = hog(img, orientations=9, pixels_per_cell=(32, 32),\n",
    "                    cells_per_block=(1,1), visualise=False)\n",
    "            for h in hist:\n",
    "                fd = np.append(fd, h)\n",
    "            if(useList[0]):\n",
    "                fd = np.append(fd, [np.array(detector.keypoints).flatten()])\n",
    "            if(useList[1]):\n",
    "                fd = np.append(fd, detector2.keypoints)\n",
    "            if(useList[2]):\n",
    "                fd = np.append(fd, edgeExtract(img, 100))\n",
    "            l1 = len(fd)\n",
    "            corners =  corner_peaks(corner_harris(img),min_distance=1)\n",
    "            if(useList[3]):\n",
    "                fd = np.append(fd, corners)\n",
    "            lengthV.append(len(fd))  \n",
    "            DataSet.append(fd)\n",
    "            ind = classes.index(className)\n",
    "            LabelSet.append(ind)\n",
    "    max = np.amax(lengthV)\n",
    "    lengthV = []\n",
    "    DataSet2 = []\n",
    "    for d in DataSet:\n",
    "        d = np.pad(d, (0, max - len(d)), 'constant')\n",
    "        DataSet2.append(d)\n",
    "        lengthV.append(len(d))\n",
    "    DataSet = DataSet2\n",
    "    res = 0\n",
    "    #perform gridsearch with one thread\n",
    "    if __name__=='__main__':\n",
    "        res = gridSearch(DataSet, LabelSet, False)\n",
    "        return res\n",
    "\n",
    "\n",
    "main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
