{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ACHAR\\Anaconda3\\lib\\site-packages\\skimage\\io\\_io.py:49: UserWarning: `as_grey` has been deprecated in favor of `as_gray`\n",
      "  warn('`as_grey` has been deprecated in favor of `as_gray`')\n",
      "C:\\Users\\ACHAR\\Anaconda3\\lib\\site-packages\\skimage\\transform\\_warps.py:105: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n",
      "C:\\Users\\ACHAR\\Anaconda3\\lib\\site-packages\\skimage\\transform\\_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.\n",
      "  warn(\"Anti-aliasing will be enabled by default in skimage 0.15 to \"\n",
      "C:\\Users\\ACHAR\\Anaconda3\\lib\\site-packages\\skimage\\feature\\_hog.py:150: skimage_deprecation: Default value of `block_norm`==`L1` is deprecated and will be changed to `L2-Hys` in v0.15. To supress this message specify explicitly the normalization method.\n",
      "  skimage_deprecation)\n",
      "C:\\Users\\ACHAR\\Anaconda3\\lib\\site-packages\\skimage\\feature\\_hog.py:248: skimage_deprecation: Argument `visualise` is deprecated and will be changed to `visualize` in v0.16\n",
      "  'be changed to `visualize` in v0.16', skimage_deprecation)\n",
      "C:\\Users\\ACHAR\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:117: VisibleDeprecationWarning: Passing `normed=True` on non-uniform bins has always been broken, and computes neither the probability density function nor the probability mass function. The result is only correct if the bins are uniform, when density=True will produce the same result anyway. The argument will be removed in a future version of numpy.\n",
      "C:\\Users\\ACHAR\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:118: VisibleDeprecationWarning: Passing `normed=True` on non-uniform bins has always been broken, and computes neither the probability density function nor the probability mass function. The result is only correct if the bins are uniform, when density=True will produce the same result anyway. The argument will be removed in a future version of numpy.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The test_size = 1 should be greater or equal to the number of classes = 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-b28964238a8c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 207\u001b[1;33m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    208\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-b28964238a8c>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     97\u001b[0m     \u001b[1;31m#perform a grid search with maximum number of possible threads (usually 4)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m         \u001b[0mgridSearch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDataSet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLabelSet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    100\u001b[0m     \u001b[1;31m#train and examine svm with default values for comparison later\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m     \u001b[0mclf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSVC\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkernel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'rbf'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mC\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1.0000000000000001e-09\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-b28964238a8c>\u001b[0m in \u001b[0;36mgridSearch\u001b[1;34m(DataSet, LabelSet, verbalize)\u001b[0m\n\u001b[0;32m    139\u001b[0m         \u001b[0mgrid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSVC\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m     \u001b[1;31m#find optimal values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m     \u001b[0mgrid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDataSet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLabelSet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m     \u001b[1;31m#print results of test for optimal values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m     \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mverbalize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    720\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresults_container\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 722\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    723\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    724\u001b[0m         \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresults_container\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1189\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1190\u001b[0m         \u001b[1;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1191\u001b[1;33m         \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1192\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1193\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[1;34m(candidate_params)\u001b[0m\n\u001b[0;32m    709\u001b[0m                                \u001b[1;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    710\u001b[0m                                in product(candidate_params,\n\u001b[1;32m--> 711\u001b[1;33m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[0;32m    712\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    713\u001b[0m                 \u001b[0mall_candidate_params\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcandidate_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py\u001b[0m in \u001b[0;36msplit\u001b[1;34m(self, X, y, groups)\u001b[0m\n\u001b[0;32m   1313\u001b[0m         \"\"\"\n\u001b[0;32m   1314\u001b[0m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroups\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindexable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1315\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iter_indices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1316\u001b[0m             \u001b[1;32myield\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1317\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py\u001b[0m in \u001b[0;36m_iter_indices\u001b[1;34m(self, X, y, groups)\u001b[0m\n\u001b[0;32m   1705\u001b[0m             raise ValueError('The test_size = %d should be greater or '\n\u001b[0;32m   1706\u001b[0m                              \u001b[1;34m'equal to the number of classes = %d'\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1707\u001b[1;33m                              (n_test, n_classes))\n\u001b[0m\u001b[0;32m   1708\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1709\u001b[0m         \u001b[1;31m# Find the sorted list of instances for each class:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: The test_size = 1 should be greater or equal to the number of classes = 2"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from sklearn import svm\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from skimage import io, filters\n",
    "from skimage.transform import resize\n",
    "from skimage.feature import hog, ORB, CENSURE, corner_peaks, BRIEF\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "\n",
    "#enter fruit names here: Choose from apple, banana, pineapple, kiwi\n",
    "#example:\n",
    "classes = ['banana', 'pineapple']\n",
    "\n",
    "def main():\n",
    "    DataSet = []\n",
    "    LabelSet = []\n",
    "    lengthV = []\n",
    "    trainPaths = ['./fruit/'+c+ '_train/' for c in classes ]\n",
    "    testPaths =  ['./fruit/'+c+' test/'   for c in classes ]\n",
    "    \n",
    "    resList = []\n",
    "    boolList = []\n",
    "    pos = 0\n",
    "    ind = 0\n",
    "    #if you wish to automatically perform both feature selection optimzation and svm optimization at the same time\n",
    "    #comment out next line and comment in section above\n",
    "    #Warning: Very long runtime for algorithm because of grid search\n",
    "    useList = [True, True, True, True]\n",
    "    #print(useList)\n",
    "    \n",
    "    for c in range(len(classes)):\n",
    "        #get label for features to be added\n",
    "        className = classes[c]\n",
    "        #get file path for folder with images\n",
    "        path = trainPaths[c]\n",
    "        #initialize feature detectors/extractors\n",
    "        #Censure extractor\n",
    "        detector = CENSURE()\n",
    "        #ORB extractor\n",
    "        detector2 = ORB(n_keypoints=50)\n",
    "        #get all file names from the folder\n",
    "        files = os.listdir(path)\n",
    "        nfiles = len(files)\n",
    "        #repeat for each file\n",
    "        for i in range(nfiles):\n",
    "            #initialize feature vector as empty list\n",
    "            featureVector = []\n",
    "            infile = files[i]\n",
    "            #read image as grayscale numpy.ndarray\n",
    "            img = io.imread(path+infile, as_grey=True)\n",
    "            #get histogram for grayscale value intensity\n",
    "            hist = np.histogram(img, bins=256)\n",
    "            #resize image\n",
    "            img = resize(img, (400,400))\n",
    "            #extract features but do not yet add them to feature vector\n",
    "            detector2.detect_and_extract(img)\n",
    "            #extract HOG features, add them to featurevector\n",
    "            a = fd = hog(img, orientations=9, pixels_per_cell=(32, 32),\n",
    "                    cells_per_block=(1,1), visualise=False)\n",
    "            #add histogramm to featurevector\n",
    "            for h in hist:\n",
    "                fd = np.append(fd, h)\n",
    "            #if corresponding boolean in uselist is true add features to featureVector --> Feature selection happens here\n",
    "            if(useList[0]):                            \n",
    "                detector.detect(img)\n",
    "                fd = np.append(fd, [np.array(detector.keypoints).flatten()])\n",
    "            if(useList[1]):\n",
    "                fd = np.append(fd, detector2.keypoints)\n",
    "            if(useList[2]):\n",
    "                fd = np.append(fd, edgeExtract(img, 100))\n",
    "            if(useList[3]):\n",
    "                corners =  corner_peaks(corner_harris(img),min_distance=1)\n",
    "                fd = np.append(fd, corners)\n",
    "            #get length of featurevector for later operations\n",
    "            lengthV.append(len(fd))\n",
    "            #add featureVector list to dataset that is fed into svm\n",
    "            DataSet.append(fd)\n",
    "            #get label name\n",
    "            ind = classes.index(className)\n",
    "            #add label to label dataset that is fed into svm\n",
    "            LabelSet.append(ind)\n",
    "    #get length of biggest sized featurevector\n",
    "    max = np.amax(lengthV)\n",
    "    lengthV = []\n",
    "    DataSet2 = []\n",
    "    #pad dataset with zeroes so that all featurevectors have the same length --> important for svm\n",
    "    for d in DataSet:\n",
    "        d = np.pad(d, (0, max - len(d)), 'constant')\n",
    "        DataSet2.append(d)\n",
    "        lengthV.append(len(d))\n",
    "    DataSet = DataSet2\n",
    "    #perform a grid search with maximum number of possible threads (usually 4)\n",
    "    if __name__=='__main__':\n",
    "        gridSearch(DataSet, LabelSet)\n",
    "    #train and examine svm with default values for comparison later\n",
    "    clf = svm.SVC(kernel='rbf', C=10.0, gamma=1.0000000000000001e-09)\n",
    "    clf.fit(DataSet, LabelSet)\n",
    "    joblib.dump(clf, classes[0]+' '+ classes[1]+'.pk1')\n",
    "    scores = cross_val_score(clf, DataSet, LabelSet, cv=10)\n",
    "    #print results of default svm\n",
    "    print(scores)\n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "#extract edge histogramm, bins number = 100    \n",
    "def edgeExtract(img, bins):\n",
    "    retVal = []\n",
    "    #apply vertical and horizontal sobel filters to get two histogramms, once of vertical and once of horizontal edges\n",
    "    #vertical\n",
    "    fs = filters.sobel_v(img)\n",
    "    #horizontal\n",
    "    angs = filters.sobel_h(img)\n",
    "    #compute histograms\n",
    "    lhist = np.histogram(fs,bins,normed=True,range=(0,1))\n",
    "    ahist = np.histogram(angs, bins,normed=True,range=(-180,180))\n",
    "    #fuse histograms into one list\n",
    "    retVal.extend(lhist[0].tolist())\n",
    "    retVal.extend(ahist[0].tolist())\n",
    "    return retVal\n",
    "#Perform grid search, \n",
    "# if optimum feature selection list is to be found verbalize = false\n",
    "def gridSearch(DataSet, LabelSet, verbalize = True):\n",
    "    #define logspace/interval from which c and gamma valuest are computed and saved to a dictionary to be passed as a parameter\n",
    "    #c from 1e-2 to 1e10\n",
    "    C_range = np.logspace(-2, 10, 13)\n",
    "    #gamma from 1e-9 to 1e3\n",
    "    gamma_range = np.logspace(-9, 3, 13)\n",
    "    param_grid = dict(gamma=gamma_range, C=C_range)\n",
    "    #grid input parameter\n",
    "    cv = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\n",
    "    #perform grid search with multiple threads for added performance\n",
    "    if(verbalize):\n",
    "        grid = GridSearchCV(SVC(), param_grid=param_grid, cv=cv, n_jobs=-1)\n",
    "    #perform grid search with one thread and return value --> multiple threads do not work with return values\n",
    "    else:\n",
    "        grid = GridSearchCV(SVC(), param_grid=param_grid, cv=cv, n_jobs=1)\n",
    "    #find optimal values\n",
    "    grid.fit(DataSet, LabelSet)\n",
    "    #print results of test for optimal values\n",
    "    if(verbalize):\n",
    "        print(\"The best parameters are %s with a score of %0.2f\" \n",
    "            % (grid.best_params_, grid.best_score_))\n",
    "    else:\n",
    "        #return best scores for different feature selections\n",
    "        return grid.best_score_\n",
    "#same as main function, however a sample of 100 images is drawn for each image\n",
    "def selectFeatures(useList):\n",
    "    DataSet = []\n",
    "    LabelSet = []\n",
    "    lengthV = []\n",
    "    trainPaths = ['./fruit/'+c+ '_train/' for c in classes ]\n",
    "    testPaths =  ['./fruit/'+c+' test/'   for c in classes ]\n",
    "    for c in range(len(classes)):\n",
    "        className = classes[c]\n",
    "        path = trainPaths[c]\n",
    "        detector = CENSURE()\n",
    "        detector2 = ORB(n_keypoints=50)\n",
    "        detector3 = BRIEF(patch_size=49)\n",
    "        files = os.listdir(path)\n",
    "        #sample\n",
    "        files = random.sample(files, 100)\n",
    "        nfiles = len(files)\n",
    "        for i in range(nfiles):\n",
    "            featureVector = []\n",
    "            infile = files[i]\n",
    "            img = io.imread(path+infile, as_grey=True)\n",
    "            hist = np.histogram(img, bins=256)\n",
    "            img = resize(img, (400,400))\n",
    "            detector2.detect_and_extract(img)\n",
    "            detector.detect(img)\n",
    "            a = fd = hog(img, orientations=9, pixels_per_cell=(32, 32),\n",
    "                    cells_per_block=(1,1), visualise=False)\n",
    "            for h in hist:\n",
    "                fd = np.append(fd, h)\n",
    "            if(useList[0]):\n",
    "                fd = np.append(fd, [np.array(detector.keypoints).flatten()])\n",
    "            if(useList[1]):\n",
    "                fd = np.append(fd, detector2.keypoints)\n",
    "            if(useList[2]):\n",
    "                fd = np.append(fd, edgeExtract(img, 100))\n",
    "            l1 = len(fd)\n",
    "            corners =  corner_peaks(corner_harris(img),min_distance=1)\n",
    "            if(useList[3]):\n",
    "                fd = np.append(fd, corners)\n",
    "            lengthV.append(len(fd))  \n",
    "            DataSet.append(fd)\n",
    "            ind = classes.index(className)\n",
    "            LabelSet.append(ind)\n",
    "    max = np.amax(lengthV)\n",
    "    lengthV = []\n",
    "    DataSet2 = []\n",
    "    for d in DataSet:\n",
    "        d = np.pad(d, (0, max - len(d)), 'constant')\n",
    "        DataSet2.append(d)\n",
    "        lengthV.append(len(d))\n",
    "    DataSet = DataSet2\n",
    "    res = 0\n",
    "    #perform gridsearch with one thread\n",
    "    if __name__=='__main__':\n",
    "        res = gridSearch(DataSet, LabelSet, False)\n",
    "        return res\n",
    "\n",
    "\n",
    "main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
